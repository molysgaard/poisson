\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cancel} % for å kunne stryke ledd
\usepackage{amsmath} % for align* og matte generelt
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[strict]{changepage}
\usepackage{bm}% for bold matte
\usepackage{subcaption}
\usepackage{xfrac} % for å kunne skrive 2/3 brøker
\usepackage{hyperref} % for URLS


\newcommand{\D}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}
\newcommand{\dd}[2]{\ensuremath{\frac{\partial^2 #1}{\partial #2^2}}}
\newcommand{\ddd}[2]{\ensuremath{\frac{\partial^3 #1}{\partial #2^3}}}

\title{Semester project \\ TMA4212 Numerical solution of differential equations by difference methods }
\author{Candidate no. 10059,10044 and 10012.}
\date{April 2013}

\begin{document}

\maketitle

\section{Problem description and motivation}
We're interested in discretizing and simulating the general wave equation

\section{Journal}
\begin{enumerate}
	\item Started with Haskell. Managed to create a sequential that was on par with C and had much better readability and consiseness.
	\item Started looking at the accelerate library for Haskell. Managed to implement a matrix multiply in it but it lacked performance because of the non-flat nature of the matrix multiply algorithm in accelerate. This made it solver than a sequential C implementation.
	\item Contacted author of accelerate to see if there was any way to mend the problems but it turned out to be a limitation in the embedded language that accelerate is. There is no way to express the type of computation that matrix multiply is effeciently. This is because of the intermidiat dot-product you calculate when doing matrix multiply. Accelerate is extremely good at things that it's made for, like stencil computations. This was also looked at but because the application was to be solving extremely large linear systems they would not fit in the memory of one GPU.
	\item First version of the SUMMA algorithm used FORTAN routines for the matrix multipyl and array copying. Since FORTRAN ararys are column-major instead of row-major as in C this lead to unpredictable and really hard to debug problems in the algorithm.
	\item Scattering and gathering the matrices from the master node proved prone to error. Not because of bad libraries or programming errors. The challenge was creating the correct mapping in a general case from a contigous array to smaller blocked contigous arrays at the low abstraction level that C works.
	\item Implementing a really fast sequential version is not trivial since most of the tools for fast numerical code, BLAS, LAPACK etc. do paralellization without you knowing it. For example the first sequential version of the program used the \verbatim{cblas_dgemm} routine. When this routine was profiled it was experienced that the routine forked off several threads to do it's work, thus these library functions could not be used for a sequential implementation.
\end{enumerate}


\begin{thebibliography}{9}

\bibitem{engquist_majda}
  Bjørn Engquist and Andrew Majda (1977).
  \emph{Absorbing Boundary Conditions for the Numerical Simulation of Waves}
  (Mathematics of Computation, Volume 31, Number 139,
  July 1977, Pages 629-651).

\bibitem{note}
  Brynjulf Owren
  \emph{TMA4212 Numerical solution of partial differential equations with finite difference methods}
  (February 24, 2011).

\bibitem{blas}
  \url{http://www.netlib.org/blas/}

\bibitem{lapack}
  \url{http://www.netlib.org/lapack/}

\end{thebibliography}

\end{document}

