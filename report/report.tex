\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cancel} % for å kunne stryke ledd
\usepackage{amsmath} % for align* og matte generelt
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[strict]{changepage}
\usepackage{bm}% for bold matte
\usepackage{subcaption}
\usepackage{xfrac} % for å kunne skrive 2/3 brøker
\usepackage{hyperref} % for URLS


\newcommand{\D}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}
\newcommand{\dd}[2]{\ensuremath{\frac{\partial^2 #1}{\partial #2^2}}}
\newcommand{\ddd}[2]{\ensuremath{\frac{\partial^3 #1}{\partial #2^3}}}

\title{Semester project \\ TMA4212 Numerical solution of differential equations by difference methods }
\author{Candidate no. 10059,10044 and 10012.}
\date{April 2013}

\begin{document}

\maketitle

\section{introduction}
The first use of comptuers was for doing simple calculations at a higher rate
than a human possibly could. Today computers are no longer just calculators but
present everywhere around us, intergrating to our lives to a bigger and bigger extent.
Even though we have past the era of big calculators scientific computing and the use
of computers for doing ever bigger calculations is just increasing. Today it is
often much cheaper to do numerical experiments, simulation the nature, instead of
setting up a real experiment. Numerical experiments are usually cheaper, less
dangorous and easier to analyse the results from. Fields that use this extencively
are military, for nuclear explosion simulations and metrologists for simulation the weather.

Almost all computer aided design is today tested numerically before any physical model is made.
Planes are tested for their flight caractheristics, cars for their aeorodynamic properties, enignes
for their thermodynamics etc. All of these applications are very computational intensive and on the
really big scale, like weather forcasts, they require immense computational power, clever algorithms
and programming techniques. This report covers a sample problem in scientific computing. Solving the
Poisson equation for a big problem size. This requires an approach very different from a naive
sequential implementation to get good performance.

\section{Problem description}
\label{sec:sol_steps}
Poisson's equation is given as
\[
-\Delta u = f
\]
where $\Delta = \nabla^2$ is the Laplace operator, u and f are real or complex valued functions in a
euclidian space. The equation is often written as
\[
-\nabla^2 u = f.
\]
In two dimensions the equation can be written as
\[
-\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) u(x,y) = f(x,y).
\]

In section \ref{sec:math_der} we derive the chosen solution method of this equation. The mathematics are
lengthy and it suffices for the parallel analyzis to know that one ends up with a three step solution
process.

\paragraph{step 1}
Form the matrix matrix product
\[
\tilde{G} = Q^\top G Q.
\]
\paragraph{step 2}
solve
\[
\Lambda \tilde{U} + \tilde{U} \Lambda = \tilde{G}
\]
or
\[
\tilde{u}_{i,j} = \frac{\tilde{g}_{i,j}}{\lambda_i + \lambda_j}.
\]
\paragraph{step 3}
Compute the matrix matrix product
\[
U = Q \tilde{U} Q^\top.
\]

Where $G$ and $U$ is a discrete version of $h^2 f$ and $u$ respectively, $Q$ is the eigen vectors of the Laplace operator,
$\lambda$ is a diagonal matrix of the eigen values of the discretized Laplace operator and $\tilde{A} = Q^\top A Q$.

\section{Computational complexity}
The asymptotic complexity of this method is bounded by the most expensive step
in the algorithm.
Given a 2D grid of size $n-1 \times n-1$ we can denote the following complexity to each step.
\paragraph{step 1}
2 matrix matrix products each have complexity $O(n^3)$.
\paragraph{step 2}
$n^2$ constant opperations, $O(n^2)$.
\paragraph{step 3}
2 matrix matrix products each have complexity $O(n^3)$.

This means that this solution method has $O(n^3)$ complexity and a serial implementation
will have asymptotic running time of $n^3$ of the problem size $n$. Here the problem size
\[
n \leq \max(n_x,n_y)
\]
where $n_x$ and $n_y$ is the number of grid points in our finite difference approximation
in the x and y direction respectively.

\section{Survey of alternate solution method}
The simplest method of solving the Poisson equation is to apply a 5 point stencil on your domain
iteratively. This means that each cell in the grid gives a fraction of its own value to its neighbors.
Then this is done for each cell iteratively untill the grid does not change more than a given threshold.

This method has the same asymptotic operation cost, $O(n^3)$ and better memory requirement $O(n^2)$.
However it does not lend itself well to paralellization. To paralellize this one would split
up the domain in blocks that each node can take care of. The five point stencil would then be
run in paralell on each of the nodes. At the end of each iteration phase a global
syncronization would have to occur for all the nodes in the compute grid to know the boundary values
to its own domain. This would lead to $O(n)$ all to all communcation steps each of a cost of
\[
4\frac{n}{\sqrt{p}}
\]
elements, where $p$ is the number of compute nodes the domain is distributed on.
This syncronization would make the algorithm way to slow for any practical size
of the problem compared to the direct diagonalization method.

\section{Analysis of paralellizable sections.}
The presented solution method (section \ref{sec:sol_steps}) is not a obvious candidate for paralellization
since a large majority of the time is used doing matrix multiplications.
This means that to make this algorithm scale well a large scale paralellized
matrix multiply has to implemented. A naive paralellization on a single SMP machine
was rejected as it would not let the algoritm scale to the huge scales that are realistic
in applications such as weather forcasts.
After some research it was found that that there exists a algorithm for this, \cite{summa}, discovered in 1997.

\subsection{The SUMMA algorithm}
TODO: Explain the SUMMA algorithm

\section{Implementations}
Two different implementations of the algorithm was done.
This was to compare the running time of the parallell version with a reference
serial implementation.

\subsection{Serial implementation}
The serial version was implemented only using vanilla C. It uses no other libraries to
do the heavy lifting in math. The reason for not using any libraries is that most BLAS and LAPACK
implementations for supercomputers are using SIMD as well as thread level paralellism. This makes
it very hard to implement a true serial program when using other libraries since these almost always
link to some underlying BLAS or LAPACK routine which is paralellized. The serial and paralell code
share the same initialization and logic. The only difference is the calls to BLAS and the SUMMA algorithm.

\subsection{Parallel implementation}
In the paralell implementation ``everything is allowed''. That is any library that could give
increased performance would be used. The following tactic was developed.

Use the SUMMA algorithm to distribute the work across different nodes. Use intel MKLs cblas implementation
to do the internal matrix multiplications in the SUMMA algorithm.

There are three main reasons to spread the work accross different nodes.
\subparagraph{Memory requirements}
As the problem size, $n$, increases the memory requirement increases at $O(n^2)$. This means that for
big problem sizes a single nodes memory is simply not enough to hold the whole problem.
\subparagraph{Cache}
Distributing the work over several nodes gives more cache memory available. Even though a problem
of a given size may not fill the available memory on a node it will wery quickly fill the available
cache. Sinche the cache memory gives several orders of magnitude faster access than normal memory
the algorithm speeds up considerably from this.
\subparagraph{Processors}
Even the biggest nodes in the QUT HPC machine only have 32 processors. Using only SMP technicques
the maximum number of processors available is thus 32. This is enough for most applications but
for this specific one where massize scalability is the key it is not enough.

The reason for using intels MKL library for the internal matrix multiplication are manyfold. First
off it is the industry leading library for math kernels. After talks with the HPC staff at QUT it was
discovered that MKL would normally outperform CUDAS BLAS, CUBLAS, because of the highly specialized code
in MKL written for exactly the processor architectures that QUTs super computer uses. Intels MKL coupled
with intels C compiler, icc, also supports linking options ta make it run in thread parallel mode. This
made it possible to accurately control the granulatiy of both the distributed memory, MPI, part and the
shared memory, Intel MKL part, to gain the highes possible performance.

\section{Wall time analyzis}

\subsection{What was measured}
what is the timing talking about?
mention that the generation of the input matrices is not in the timing, nor is the
generation of the eigenvalues/vectors for the discrete laplace opperator.

\subsection{Serial implementation runtime}

\subsection{Parallel implementation run time}
plot the run time for a paralell implementation

\subsection{Parallel CUDA implementation run time}
plot the run time for a CUDA paralell implementation

\subsection{Speedup}
plot speedup


\section{Journal}
\begin{enumerate}
	\item Started with Haskell. Managed to create a sequential that was on par with C and had much better readability and consiseness.
	\item Started looking at the accelerate library for Haskell. Managed to implement a matrix multiply in it but it lacked performance because of the non-flat nature of the matrix multiply algorithm in accelerate. This made it solver than a sequential C implementation.
	\item Contacted author of accelerate to see if there was any way to mend the problems but it turned out to be a limitation in the embedded language that accelerate is. There is no way to express the type of computation that matrix multiply is effeciently. This is because of the intermidiat dot-product you calculate when doing matrix multiply. Accelerate is extremely good at things that it's made for, like stencil computations. This was also looked at but because the application was to be solving extremely large linear systems they would not fit in the memory of one GPU.
	\item First version of the SUMMA algorithm used FORTAN routines for the matrix multipyl and array copying. Since FORTRAN ararys are column-major instead of row-major as in C this lead to unpredictable and really hard to debug problems in the algorithm.
	\item Scattering and gathering the matrices from the master node proved prone to error. Not because of bad libraries or programming errors. The challenge was creating the correct mapping in a general case from a contigous array to smaller blocked contigous arrays at the low abstraction level that C works.
	\item Implementing a really fast sequential version is not trivial since most of the tools for fast numerical code, BLAS, LAPACK etc. do paralellization without you knowing it. For example the first sequential version of the program used the cblas\_dgemm routine. When this routine was profiled it was experienced that the routine forked off several threads to do it's work, thus these library functions could not be used for a sequential implementation.
\end{enumerate}

\section{Mathematical derivation of solution method}
\label{sec:math_der}
Poisson's equation is given as
\[
-\Delta u = f
\]
where $\Delta = \nabla^2$ is the Laplace operator, u and f are real or complex valued functions in a
euclidian space. The equation is often written as
\[
-\nabla^2 u = f.
\]
In two dimensions the equation can be written as
\[
-\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) u(x,y) = f(x,y).
\]

This equation is continous and to work with it in the computer some discretized approximation has
to be made. This equation is continous and to work with it in the computer some discretized approximation has to be made. A finite difference approximation was chosen as it presents us with a
problem that have several solution strategies that illustrates well the important properties to
consider when designing parallel numerical algorithms.

After a FDM discretization we are left with a 2D regular grid. Each node in this grid represents
a point for which we compute an approximation to our functions $u(x,y)$ and $f(x,y)$.
Using a central difference approximation to the partial derivatives in each direction leaves us
with the following equation
\[
-\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2} - \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2} = f_{i,j}, 1\leq i,j \leq n-1.
\]

The goal is to express this 2D equation as a linear system of equations which we then can solve with
different techniques using the computer.

Let
\begin{equation}
       U = \begin{bmatrix}
               u_{1,1} & \dots & u_{1,n} \\
               \vdots  &       & \vdots \\
               u_{n,1} & \dots & u_{n,n}
       \end{bmatrix}
\end{equation}
be the discretized version of $u$.

Let 
\begin{equation}
       T = \begin{bmatrix}
               2 & -1 & & & \\
               -1 & 2 & -1 & & \\
               & \ddots & \ddots & \ddots &\\
               & & -1 & 2 & -1\\
               & & & -1 & 2 \\
       \end{bmatrix}
\end{equation}
be the discrete partial double derivative operator.
Then,
\begin{align*}
       (TU)_{i,j} &= 2u_{i,j} - u_{i+1,j}, &i=1,\\
       (TU)_{i,j} &= 2u_{i,j} - u_{i+1,j} - u_{i-1,j}, &2 \leq i \leq n-2,\\
       (TU)_{i,j} &= 2u_{i,j} - u_{i-1,j}, &i=n-1.\\
\end{align*}
and thus,
\[
\frac{1}{h^2}(TU)_{i,j} \approx - \left(\frac{\partial^2 u}{\partial x^2}\right)_{i,j}.
\]
By the same argument
\[
\frac{1}{h^2}(UT)_{i,j} \approx - \left(\frac{\partial^2 u}{\partial y^2}\right)_{i,j}.
\]

Our finite difference scheme can thus be written as
\[
\frac{1}{h^2}(TU + UT)_{i,j} = f_{i,j}, \quad 1\leq i,j \leq n-1.
\]
Or
\[
\label{linsys}
TU + UT = G
\]
where
\begin{equation}
G = h^2 \begin{bmatrix}
               f_{1,1} & \dots & f_{1,n} \\
               \vdots  &       & \vdots \\
               f_{n,1} & \dots & f_{n,n}
       \end{bmatrix}
\end{equation}.

The $T$ matrix may be diagonalized
\[
T = Q \Lambda Q^\top
\]
where $\Lambda$ is a diagonal matrix and $Q Q^\top=I$, the identity matrix.
When we insert this expression for $T$ in \eqref{linsys} we get
\[
Q \Lambda Q^\top U + U Q \Lambda Q^\top = G.
\]
Multiplying from rigth with $Q$ and left with $Q^\top$ gives
\begin{align*}
&(Q^\top Q) \Lambda Q^\top U Q + Q^\top U Q \Lambda (Q^\top Q)\\
&= \Lambda Q^\top U Q + Q^\top U Q \Lambda = Q^\top G Q.
\end{align*}


\begin{thebibliography}{9}

\bibitem{summa}
  Robert A. van de Geijn and Jerrell Watts (1997)
  \emph{SUMMA: Scalable Universal Matrix Multiplication Algorithm}
  \url{http://www.netlib.org/lapack/lawnspdf/lawn96.pdf}

\bibitem{blas}
  \url{http://www.netlib.org/blas/}

\bibitem{lapack}
  \url{http://www.netlib.org/lapack/}

\end{thebibliography}

\end{document}

